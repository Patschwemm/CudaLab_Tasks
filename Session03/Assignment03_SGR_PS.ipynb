{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGxxg_M8g82I"
      },
      "source": [
        "# Assignment 03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4jg_j49RHtBu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import utils\n",
        "import numpy as np\n",
        "import random\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZctjS30eHtBy"
      },
      "outputs": [],
      "source": [
        "utils.set_random_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_BT5reACHtBz"
      },
      "outputs": [],
      "source": [
        "### Setting up \"constants\", num Labels for layer sizes, and network outputs\n",
        "NUM_LABELS = 196\n",
        "BATCH_SIZE = 24\n",
        "LR = 3e-4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_EPOCHS = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSxZo8jBg82Z"
      },
      "source": [
        "### **Augmentations**\n",
        "\n",
        "For your experiments, use augmentations from the following types:\n",
        "- Spatial Augmentations (rotation, mirroring, croppoing, ...)\n",
        "- Use some other augmentations (color jitter, gaussian noise, ...).\n",
        "- Use one (or more) of the following advanced augmentations:\n",
        "   - **CutMix**: https://arxiv.org/pdf/1905.04899.pdf\n",
        "   - **Mixup**: https://arxiv.org/pdf/1710.09412.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1gHcC7F4HtB0"
      },
      "outputs": [],
      "source": [
        "# these spatial transforms seemed to work better than with color transforms\n",
        "\n",
        "simple_transforms = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize((224,224)),\n",
        "    T.RandomHorizontalFlip(p=0.25),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    # T.RandomResizedCrop(224),\n",
        "    # T.GaussianBlur(kernel_size=(5,5)),\n",
        "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
        "])\n",
        "\n",
        "resize_test = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize((224,224)),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dIdsTVrFHtB1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "train_set = datasets.StanfordCars(\n",
        "    \"./stanfordcars/\", \n",
        "    split=\"train\", \n",
        "    transform=simple_transforms, \n",
        "    download=True\n",
        ")\n",
        "test_set = datasets.StanfordCars(\n",
        "    \"./stanfordcars/\", \n",
        "    split=\"test\", \n",
        "    transform=resize_test,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False, \n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9_QWNNUg82f"
      },
      "source": [
        "### Cutmix Implementation according to their paper, spatial augmentations seemed to work better. Initial training with color and blur seemed to perform worse, therefore we omitted them for a final training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9UYf9b7lHtB5"
      },
      "outputs": [],
      "source": [
        "# cutmix implementation, implemented from pseudo code given by the paper\n",
        "def CutMix(input, target):\n",
        "    # get batch size to shuffle index of minibatch, and set a shuffled list\n",
        "    B = list(range(input.shape[0]))\n",
        "    shuffled_batch_idx = torch.tensor(random.sample(B, len(B)))\n",
        "\n",
        "    # alg according to paper\n",
        "    W = input[0].shape[1]\n",
        "    H = input[0].shape[2]\n",
        "    Lambda = torch.rand(1)\n",
        "    r_x = torch.rand(1) * H\n",
        "    r_y = torch.rand(1) * W\n",
        "    r_w = torch.sqrt(1 - Lambda) * H\n",
        "    r_h = torch.sqrt(1 - Lambda) * W\n",
        "\n",
        "    x1 = int(torch.clamp((r_x - r_w / 2), min=0, max=W))\n",
        "    x2 = int(torch.clamp((r_x + r_w / 2), min=0, max=W))\n",
        "    y1 = int(torch.clamp((r_y - r_h / 2), min=0, max=H))\n",
        "    y2 = int(torch.clamp((r_y + r_h / 2), min=0, max=H))\n",
        "    \n",
        "    # target = Lambda * target + (1 - Lambda) * target[shuffled_batch_idx]\n",
        "    Lambda = 1 - ((x2-x1) * (y2 - y1) / (W*H))\n",
        "    input[:, :, y1:y2, x1:x2] = input[shuffled_batch_idx, :, y1:y2, x1:x2]\n",
        "\n",
        "    return input, target, shuffled_batch_idx, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oMEo1jwLHtB7"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, epoch, device=DEVICE):\n",
        "    \"\"\" Training a model for one epoch \"\"\"\n",
        "    \n",
        "    loss_list = []\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for i, (images, labels) in progress_bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        prob_cutmix = torch.rand(1)\n",
        "        if prob_cutmix > 0.75:\n",
        "            # used for cutmix agumentation\n",
        "            images, labels, shuffled_idx, Lambda = CutMix(images, labels)\n",
        "            \n",
        "            # Forward pass to get output/logits\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # Calculate Loss: softmax --> cross entropy loss\n",
        "            # split loss values according to cutmix paper\n",
        "            loss = criterion(outputs, labels) * Lambda  + criterion(outputs, labels[shuffled_idx]) * (1 - Lambda)\n",
        "        else: \n",
        "\n",
        "            #compute output as usual\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "         \n",
        "        # Getting gradients w.r.t. parameter\n",
        "        loss.backward()\n",
        "         \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.3f}. \")\n",
        "        \n",
        "    \n",
        "    mean_loss = np.mean(loss_list)\n",
        "    progress_bar.set_description(f\"End Epoch {epoch}: loss {mean_loss:.3f}. \")\n",
        "    return mean_loss, loss_list\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, eval_loader, criterion, device=DEVICE):\n",
        "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_list = []\n",
        "\n",
        "    ### set up confusion matrix\n",
        "    conf_matx = np.zeros((NUM_LABELS, NUM_LABELS))\n",
        "    ### Correct = Accumulator of correctly labeled predictions\n",
        "    correct = torch.zeros(1).to(device)\n",
        "\n",
        "    \n",
        "    for images, labels in eval_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass only to get logits/output\n",
        "        outputs = model(images)\n",
        "                 \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "            \n",
        "        # Get predictions from the maximum value\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += len( torch.where(preds==labels)[0] )\n",
        "        total += len(labels)\n",
        "        conf_matx += confusion_matrix(\n",
        "            y_true=labels.cpu().numpy(), y_pred=preds.cpu().numpy(), labels=np.arange(0, NUM_LABELS, 1)\n",
        "            )\n",
        "\n",
        "                 \n",
        "    # Total correct predictions and loss\n",
        "    accuracy = correct / total * 100\n",
        "    loss = np.mean(loss_list)\n",
        "    \n",
        "    return accuracy, loss, conf_matx\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, tboard, device=DEVICE, start_epoch=0):\n",
        "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
        "    \n",
        "    train_loss = []\n",
        "    val_loss =  []\n",
        "    loss_iters = []\n",
        "    valid_acc = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        # setup for confusion matrix\n",
        "        correct=torch.zeros(1).to(DEVICE)\n",
        "\n",
        "        # validation epoch\n",
        "        model.eval()  # important for dropout and batch norms\n",
        "        accuracy, loss, _ = eval_model(model=model, eval_loader=valid_loader, criterion=criterion, device=device)\n",
        "        valid_acc.append(accuracy)\n",
        "        val_loss.append(loss)\n",
        "        tboard.add_scalar(f'Accuracy/Valid', accuracy, global_step=epoch+start_epoch)\n",
        "        tboard.add_scalar(f'Loss/Valid', loss, global_step=epoch+start_epoch)\n",
        "        \n",
        "        # training epoch\n",
        "        model.train()  # important for dropout and batch norms\n",
        "        mean_loss, cur_loss_iters = train_epoch(\n",
        "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
        "                criterion=criterion, epoch=epoch, device=device\n",
        "            )\n",
        "        scheduler.step()\n",
        "        train_loss.append(mean_loss)\n",
        "        tboard.add_scalar(f'Loss/Train', mean_loss, global_step=epoch+start_epoch)\n",
        "\n",
        "        loss_iters = loss_iters + cur_loss_iters\n",
        "        \n",
        "    print(f\"Training completed\")\n",
        "    return train_loss, val_loss, loss_iters, valid_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13O_fSu_g82o"
      },
      "source": [
        "### **Experiments:** \n",
        "#### **Experiments 1.1:**\n",
        " Using your aforementioned augmentions:\n",
        " - Fine-tune VGG, ResNet and ConvNext for your augmented dataset for car type classification and compare them.\n",
        " - Log your losses and accuracies into Tensorboard (or some other logging tool)\n",
        " - **Extra Point**: \n",
        "   - Fine-tune a Transformer-based model (e.g. ViT). Compare the performance (accuracy, confusion matrix, training time, loss landscape, ...) with the one from ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gs9h1yaHtB-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Capture, cause pretrained param has a deprecated warning, but still works\n",
        "# Loading models\n",
        "vgg_model = models.vgg16_bn(pretrained=True)\n",
        "\n",
        "vgg_model.classifier = nn.Linear(7*7*512, NUM_LABELS)\n",
        "vgg_model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZv8I5v0HtCA"
      },
      "outputs": [],
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_vgg = torch.optim.Adam(vgg_model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 5 epochs\n",
        "scheduler_vgg = torch.optim.lr_scheduler.StepLR(optimizer_vgg, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ShCCyHHtCB",
        "outputId": "b9768e0f-f6b8-46a8-959c-8f8575d91719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 128: loss 5.560. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 2 Iter 128: loss 2.195. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 3 Iter 128: loss 3.165. : 100%|██████████| 128/128 [01:50<00:00,  1.15it/s]\n",
            "Epoch 4 Iter 128: loss 0.609. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 5 Iter 128: loss 0.233. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 6 Iter 128: loss 0.209. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 7 Iter 128: loss 0.281. : 100%|██████████| 128/128 [01:50<00:00,  1.15it/s]\n",
            "Epoch 8 Iter 128: loss 0.040. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 9 Iter 128: loss 3.118. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 10 Iter 128: loss 2.412. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 11 Iter 128: loss 0.031. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 12 Iter 128: loss 0.020. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]\n",
            "Epoch 13 Iter 128: loss 0.027. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 14 Iter 128: loss 0.062. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]\n",
            "Epoch 15 Iter 128: loss 0.024. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 16 Iter 128: loss 1.660. : 100%|██████████| 128/128 [01:50<00:00,  1.15it/s]\n",
            "Epoch 17 Iter 128: loss 0.022. : 100%|██████████| 128/128 [01:50<00:00,  1.16it/s]\n",
            "Epoch 18 Iter 128: loss 0.018. : 100%|██████████| 128/128 [01:51<00:00,  1.14it/s]\n",
            "Epoch 19 Iter 128: loss 0.071. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]\n",
            "Epoch 20 Iter 128: loss 0.047. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]\n",
            "Epoch 21 Iter 128: loss 0.016. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]\n",
            "Epoch 22 Iter 128: loss 0.018. : 100%|██████████| 128/128 [01:50<00:00,  1.15it/s]\n",
            "Epoch 23 Iter 128: loss 0.055. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]\n",
            "Epoch 24 Iter 128: loss 0.015. : 100%|██████████| 128/128 [01:50<00:00,  1.15it/s]\n",
            "Epoch 25 Iter 128: loss 2.110. : 100%|██████████| 128/128 [01:51<00:00,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "writer_vgg = utils.make_tboard_logs(\"vgg_16_finetuned\")\n",
        "vgg_train_loss, vgg_val_loss, vgg_loss_iters, vgg_valid_acc = train_model(\n",
        "        model=vgg_model, optimizer=optimizer_vgg, scheduler=scheduler_vgg, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_vgg,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLAW24xzHtCE"
      },
      "outputs": [],
      "source": [
        "utils.save_model(\n",
        "    vgg_model, optimizer_vgg, NUM_EPOCHS, (vgg_train_loss, vgg_val_loss, vgg_loss_iters, vgg_valid_acc)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ytuUX5sHtCF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "resnet18_model = models.resnet18(pretrained=True)\n",
        "\n",
        "resnet18_model.classifier = nn.Linear(7*7*512, NUM_LABELS)\n",
        "resnet18_model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGJmPevmHtCG"
      },
      "outputs": [],
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_resnet18 = torch.optim.Adam(resnet18_model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 5 epochs\n",
        "scheduler_resnet18 = torch.optim.lr_scheduler.StepLR(optimizer_resnet18, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_HH1U_zGj-L",
        "outputId": "9e1474e9-2ca4-40e5-e4d0-5d80d3129957"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 128: loss 3.236. : 100%|██████████| 128/128 [00:44<00:00,  2.87it/s]\n",
            "Epoch 2 Iter 128: loss 1.510. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 3 Iter 128: loss 4.157. : 100%|██████████| 128/128 [00:44<00:00,  2.90it/s]\n",
            "Epoch 4 Iter 128: loss 0.586. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 5 Iter 128: loss 0.831. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 6 Iter 128: loss 1.078. : 100%|██████████| 128/128 [00:44<00:00,  2.87it/s]\n",
            "Epoch 7 Iter 128: loss 2.375. : 100%|██████████| 128/128 [00:45<00:00,  2.84it/s]\n",
            "Epoch 8 Iter 128: loss 0.065. : 100%|██████████| 128/128 [00:44<00:00,  2.88it/s]\n",
            "Epoch 9 Iter 128: loss 0.120. : 100%|██████████| 128/128 [00:44<00:00,  2.86it/s]\n",
            "Epoch 10 Iter 128: loss 0.366. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 11 Iter 128: loss 2.622. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 12 Iter 128: loss 0.072. : 100%|██████████| 128/128 [00:44<00:00,  2.86it/s]\n",
            "Epoch 13 Iter 128: loss 0.086. : 100%|██████████| 128/128 [00:44<00:00,  2.88it/s]\n",
            "Epoch 14 Iter 128: loss 0.047. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 15 Iter 128: loss 0.093. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 16 Iter 128: loss 2.345. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 17 Iter 128: loss 0.065. : 100%|██████████| 128/128 [00:44<00:00,  2.90it/s]\n",
            "Epoch 18 Iter 128: loss 0.097. : 100%|██████████| 128/128 [00:44<00:00,  2.88it/s]\n",
            "Epoch 19 Iter 128: loss 0.135. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 20 Iter 128: loss 0.051. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 21 Iter 128: loss 0.091. : 100%|██████████| 128/128 [00:44<00:00,  2.88it/s]\n",
            "Epoch 22 Iter 128: loss 0.061. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 23 Iter 128: loss 0.024. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]\n",
            "Epoch 24 Iter 128: loss 0.045. : 100%|██████████| 128/128 [00:44<00:00,  2.88it/s]\n",
            "Epoch 25 Iter 128: loss 0.062. : 100%|██████████| 128/128 [00:44<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "writer_resnet18 = utils.make_tboard_logs(\"resnet_18_finetuned\")\n",
        "resnet18_train_loss, resnet18_val_loss, resnet18_loss_iters, resnet18_valid_acc = train_model(\n",
        "        model=resnet18_model, optimizer=optimizer_resnet18, scheduler=scheduler_resnet18, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_resnet18,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhQei9h8hK3i"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "convnext_model = models.convnext_tiny(pretrained=True)\n",
        "convnext_model.classifier[2] = nn.Linear(768, NUM_LABELS)\n",
        "\n",
        "convnext_model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao_7IpjvhZtk"
      },
      "outputs": [],
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_convnext = torch.optim.Adam(convnext_model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 5 epochs\n",
        "scheduler_convnext = torch.optim.lr_scheduler.StepLR(optimizer_convnext, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdiV7Kg1jQIz",
        "outputId": "e1d6f195-1c64-4950-d412-96d35b27b9d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 128: loss 2.061. : 100%|██████████| 128/128 [03:12<00:00,  1.50s/it]\n",
            "Epoch 2 Iter 128: loss 1.206. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 3 Iter 128: loss 0.596. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 4 Iter 128: loss 0.131. : 100%|██████████| 128/128 [02:52<00:00,  1.35s/it]\n",
            "Epoch 5 Iter 128: loss 0.101. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 6 Iter 128: loss 0.104. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 7 Iter 128: loss 0.073. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 8 Iter 128: loss 0.027. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 9 Iter 128: loss 0.026. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 10 Iter 128: loss 0.010. : 100%|██████████| 128/128 [02:35<00:00,  1.21s/it]\n",
            "Epoch 11 Iter 128: loss 0.723. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 12 Iter 128: loss 0.922. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 13 Iter 128: loss 0.008. : 100%|██████████| 128/128 [02:31<00:00,  1.19s/it]\n",
            "Epoch 14 Iter 128: loss 0.017. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 15 Iter 128: loss 0.008. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 16 Iter 128: loss 0.105. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 17 Iter 128: loss 0.014. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 18 Iter 128: loss 0.009. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 19 Iter 128: loss 0.003. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 20 Iter 128: loss 0.005. : 100%|██████████| 128/128 [02:31<00:00,  1.19s/it]\n",
            "Epoch 21 Iter 128: loss 0.010. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 22 Iter 128: loss 1.269. : 100%|██████████| 128/128 [02:31<00:00,  1.19s/it]\n",
            "Epoch 23 Iter 128: loss 0.006. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 24 Iter 128: loss 2.222. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]\n",
            "Epoch 25 Iter 128: loss 0.009. : 100%|██████████| 128/128 [02:32<00:00,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "writer_convnext = utils.make_tboard_logs(\"convnext_tiny_finetuned\")\n",
        "convnext_train_loss, convnext_val_loss, convnext_loss_iters, convnext_valid_acc = train_model(\n",
        "        model=convnext_model, optimizer=optimizer_convnext, scheduler=scheduler_convnext, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_convnext,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NmFo3PtklNR"
      },
      "source": [
        "The ConvNeXt model (tiny) perfomed the best (92.43% accuracy) followed by the resnet18 Model (81.69% accuracy) and vgg16 with batch norm had the worst performance (78.14%). It also took the longest of them all (1.35h), vgg16 second longest (1.15h) while resnet was the fastest model (0.5h). \\\\\n",
        "\n",
        "We can see that the validation loss of the convnext model is less than the training loss while resnet and vgg slightly overfit. \n",
        "\n",
        "The modern ConvNeXt model seems to outperform the older more traditional CNNs by a big margin, because of new transformer based techniques implemented for a RESnet50 network architecture, e.g. a multi-stage designs which includes different feature map resolution etc. . These modern additions give a huge advantage to ConvNeXt with regards to ResNet and VGG. The gap between ResNet and VGG can be explained through the skip layers being added in the ResNet.\n",
        "\n",
        "The Tensorboard logs visualize these results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE31DB0Tg82w"
      },
      "source": [
        "#### **Experiments 1.2:**\n",
        " - Compare the following: Fine-Tuned ResNet, ResNet as fixed feature extractor, and ResNet with a Combined Approach\n",
        " - Log your losses and accuracies into Tensorboard (or some other logging tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6RRfTcng82x"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# resnet as fixed feature extractor\n",
        "resnet_18_fixed_model = models.resnet18(pretrained=True)\n",
        "\n",
        "for params in resnet_18_fixed_model.parameters():\n",
        "    params.requires_grad_(False)\n",
        "\n",
        "resnet_18_fixed_model.fc = nn.Linear(512, NUM_LABELS)\n",
        "\n",
        "resnet_18_fixed_model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-5Ca5p5g82y",
        "outputId": "62e4b407-cbcf-4d32-e96d-bfead635aa42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 128: loss 4.930. : 100%|██████████| 128/128 [00:51<00:00,  2.48it/s]\n",
            "Epoch 2 Iter 128: loss 4.624. : 100%|██████████| 128/128 [00:55<00:00,  2.31it/s]\n",
            "Epoch 3 Iter 128: loss 4.293. : 100%|██████████| 128/128 [00:58<00:00,  2.20it/s]\n",
            "Epoch 4 Iter 128: loss 4.072. : 100%|██████████| 128/128 [00:52<00:00,  2.42it/s]\n",
            "Epoch 5 Iter 128: loss 4.039. : 100%|██████████| 128/128 [00:59<00:00,  2.15it/s]\n",
            "Epoch 6 Iter 128: loss 3.901. : 100%|██████████| 128/128 [00:55<00:00,  2.32it/s]\n",
            "Epoch 7 Iter 128: loss 3.439. : 100%|██████████| 128/128 [00:56<00:00,  2.27it/s]\n",
            "Epoch 8 Iter 128: loss 3.079. : 100%|██████████| 128/128 [00:54<00:00,  2.36it/s]\n",
            "Epoch 9 Iter 128: loss 3.541. : 100%|██████████| 128/128 [00:55<00:00,  2.29it/s]\n",
            "Epoch 10 Iter 128: loss 4.632. : 100%|██████████| 128/128 [00:54<00:00,  2.34it/s]\n",
            "Epoch 11 Iter 128: loss 3.317. : 100%|██████████| 128/128 [00:54<00:00,  2.35it/s]\n",
            "Epoch 12 Iter 128: loss 3.478. : 100%|██████████| 128/128 [00:55<00:00,  2.30it/s]\n",
            "Epoch 13 Iter 128: loss 3.491. : 100%|██████████| 128/128 [00:55<00:00,  2.31it/s]\n",
            "Epoch 14 Iter 128: loss 3.376. : 100%|██████████| 128/128 [00:55<00:00,  2.29it/s]\n",
            "Epoch 15 Iter 128: loss 3.468. : 100%|██████████| 128/128 [00:57<00:00,  2.22it/s]\n",
            "Epoch 16 Iter 128: loss 3.421. : 100%|██████████| 128/128 [00:53<00:00,  2.38it/s]\n",
            "Epoch 17 Iter 128: loss 3.647. : 100%|██████████| 128/128 [00:54<00:00,  2.36it/s]\n",
            "Epoch 18 Iter 128: loss 3.316. : 100%|██████████| 128/128 [00:55<00:00,  2.32it/s]\n",
            "Epoch 19 Iter 128: loss 3.644. : 100%|██████████| 128/128 [00:58<00:00,  2.19it/s]\n",
            "Epoch 20 Iter 128: loss 4.547. : 100%|██████████| 128/128 [00:55<00:00,  2.32it/s]\n",
            "Epoch 21 Iter 128: loss 3.197. : 100%|██████████| 128/128 [00:55<00:00,  2.30it/s]\n",
            "Epoch 22 Iter 128: loss 3.344. : 100%|██████████| 128/128 [00:55<00:00,  2.31it/s]\n",
            "Epoch 23 Iter 128: loss 3.893. : 100%|██████████| 128/128 [00:55<00:00,  2.31it/s]\n",
            "Epoch 24 Iter 128: loss 4.052. : 100%|██████████| 128/128 [00:56<00:00,  2.27it/s]\n",
            "Epoch 25 Iter 128: loss 3.430. : 100%|██████████| 128/128 [00:53<00:00,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_resnet_18_fixed = torch.optim.Adam(resnet_18_fixed_model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 5 epochs\n",
        "scheduler_resnet_18_fixed = torch.optim.lr_scheduler.StepLR(optimizer_resnet_18_fixed, step_size=7, gamma=0.1)\n",
        "\n",
        "writer_resnet_18_fixed = utils.make_tboard_logs(\"resnet_18_fixed_finetuned\")\n",
        "resnet_18_fixed_train_loss, resnet_18_fixed_val_loss, resnet_18_fixed_loss_iters, resnet_18_fixed_valid_acc = train_model(\n",
        "        model=resnet_18_fixed_model, optimizer=optimizer_resnet_18_fixed, scheduler=scheduler_resnet_18_fixed, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_resnet_18_fixed,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCbeBajS75fU"
      },
      "outputs": [],
      "source": [
        "utils.save_model(resnet_18_fixed_model, optimizer_resnet_18_fixed, 25, (resnet_18_fixed_train_loss, resnet_18_fixed_val_loss, resnet_18_fixed_loss_iters, resnet_18_fixed_valid_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1feB48X776ml"
      },
      "outputs": [],
      "source": [
        "def optimizer_to(optim, device):\n",
        "    for param in optim.state.values():\n",
        "        # Not sure there are any global tensors in the state dict\n",
        "        if isinstance(param, torch.Tensor):\n",
        "            param.data = param.data.to(device)\n",
        "            if param._grad is not None:\n",
        "                param._grad.data = param._grad.data.to(device)\n",
        "        elif isinstance(param, dict):\n",
        "            for subparam in param.values():\n",
        "                if isinstance(subparam, torch.Tensor):\n",
        "                    subparam.data = subparam.data.to(device)\n",
        "                    if subparam._grad is not None:\n",
        "                        subparam._grad.data = subparam._grad.data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNTfiDTx77wJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# resnet as fixed feature extractor\n",
        "resnet_18_fixed_model = models.resnet18()\n",
        "resnet_18_fixed_model.fc = nn.Linear(512, NUM_LABELS)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_resnet_18_fixed = torch.optim.Adam(resnet_18_fixed_model.parameters(), lr=3e-4)\n",
        "\n",
        "resnet_18_fixed_model, optimizer_resnet_18_fixed, _, _ = utils.load_model(\n",
        "    resnet_18_fixed_model,\n",
        "    optimizer_resnet_18_fixed,\n",
        "    \"./models/checkpoint_ResNet_epoch_25.pth\"\n",
        "    )\n",
        "\n",
        "scheduler_resnet_18_fixed = torch.optim.lr_scheduler.StepLR(optimizer_resnet_18_fixed, step_size=7, gamma=0.1)\n",
        "\n",
        "resnet_18_fixed_model.to(DEVICE);\n",
        "optimizer_to(optimizer_resnet_18_fixed, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7ll15iwL6JH",
        "outputId": "aaaaf0cf-4351-4707-d9b8-733719c516a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 128: loss 1.712. : 100%|██████████| 128/128 [01:05<00:00,  1.97it/s]\n",
            "Epoch 2 Iter 128: loss 0.985. : 100%|██████████| 128/128 [01:06<00:00,  1.92it/s]\n",
            "Epoch 3 Iter 128: loss 0.656. : 100%|██████████| 128/128 [01:06<00:00,  1.91it/s]\n",
            "Epoch 4 Iter 128: loss 0.711. : 100%|██████████| 128/128 [01:08<00:00,  1.86it/s]\n",
            "Epoch 5 Iter 128: loss 0.356. : 100%|██████████| 128/128 [01:07<00:00,  1.89it/s]\n",
            "Epoch 6 Iter 128: loss 0.679. : 100%|██████████| 128/128 [01:08<00:00,  1.88it/s]\n",
            "Epoch 7 Iter 128: loss 0.112. : 100%|██████████| 128/128 [01:06<00:00,  1.92it/s]\n",
            "Epoch 8 Iter 128: loss 0.090. : 100%|██████████| 128/128 [01:07<00:00,  1.89it/s]\n",
            "Epoch 9 Iter 128: loss 0.139. : 100%|██████████| 128/128 [01:10<00:00,  1.82it/s]\n",
            "Epoch 10 Iter 128: loss 2.328. : 100%|██████████| 128/128 [01:06<00:00,  1.93it/s]\n",
            "Epoch 11 Iter 128: loss 0.077. : 100%|██████████| 128/128 [01:07<00:00,  1.90it/s]\n",
            "Epoch 12 Iter 128: loss 0.115. : 100%|██████████| 128/128 [01:06<00:00,  1.92it/s]\n",
            "Epoch 13 Iter 128: loss 0.057. : 100%|██████████| 128/128 [01:08<00:00,  1.88it/s]\n",
            "Epoch 14 Iter 128: loss 0.055. : 100%|██████████| 128/128 [01:07<00:00,  1.89it/s]\n",
            "Epoch 15 Iter 128: loss 0.038. : 100%|██████████| 128/128 [01:07<00:00,  1.90it/s]\n",
            "Epoch 16 Iter 128: loss 0.100. : 100%|██████████| 128/128 [01:07<00:00,  1.90it/s]\n",
            "Epoch 17 Iter 128: loss 0.053. : 100%|██████████| 128/128 [01:06<00:00,  1.93it/s]\n",
            "Epoch 18 Iter 128: loss 0.038. : 100%|██████████| 128/128 [01:06<00:00,  1.92it/s]\n",
            "Epoch 19 Iter 128: loss 0.072. : 100%|██████████| 128/128 [01:07<00:00,  1.88it/s]\n",
            "Epoch 20 Iter 128: loss 2.650. : 100%|██████████| 128/128 [01:08<00:00,  1.87it/s]\n",
            "Epoch 21 Iter 128: loss 0.046. : 100%|██████████| 128/128 [01:09<00:00,  1.85it/s]\n",
            "Epoch 22 Iter 128: loss 0.060. : 100%|██████████| 128/128 [01:06<00:00,  1.93it/s]\n",
            "Epoch 23 Iter 128: loss 0.126. : 100%|██████████| 128/128 [01:08<00:00,  1.88it/s]\n",
            "Epoch 24 Iter 128: loss 2.195. : 100%|██████████| 128/128 [01:07<00:00,  1.89it/s]\n",
            "Epoch 25 Iter 128: loss 0.126. : 100%|██████████| 128/128 [01:09<00:00,  1.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        }
      ],
      "source": [
        "writer_resnet_18_fixed = utils.make_tboard_logs(\"resnet_18_joint_finetuned\")\n",
        "resnet_18_fixed_train_loss, resnet_18_fixed_val_loss, resnet_18_fixed_loss_iters, resnet_18_fixed_valid_acc = train_model(\n",
        "        model=resnet_18_fixed_model, optimizer=optimizer_resnet_18_fixed, scheduler=scheduler_resnet_18_fixed, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_resnet_18_fixed,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_C5qwGBW79g1"
      },
      "source": [
        "By the results the finetuned was the best (81.69% accuracy). The classifier only approach only has an accuracy of 26%. The joint probability only gets an accuracy of 79.99% therefore still performing worse than the fine tuned resnet18."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGojBKTkl3A"
      },
      "source": [
        "## Transformer Training started: MaxVit transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNI8X3nejDX4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "maxvit_model = models.maxvit_t(weights=models.MaxVit_T_Weights.DEFAULT)\n",
        "\n",
        "maxvit_model.classifier[5] = nn.Linear(512, NUM_LABELS)\n",
        "\n",
        "maxvit_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRR2HjxKsPam",
        "outputId": "04081c80-ba96-4b3b-abdc-a13fdd5054e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 340: loss 2.659. : 100%|██████████| 340/340 [02:07<00:00,  2.67it/s]\n",
            "Epoch 2 Iter 340: loss 3.281. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 3 Iter 340: loss 0.702. : 100%|██████████| 340/340 [02:08<00:00,  2.64it/s]\n",
            "Epoch 4 Iter 340: loss 0.208. : 100%|██████████| 340/340 [02:09<00:00,  2.64it/s]\n",
            "Epoch 5 Iter 340: loss 3.734. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 6 Iter 340: loss 3.342. : 100%|██████████| 340/340 [02:07<00:00,  2.67it/s]\n",
            "Epoch 7 Iter 340: loss 0.496. : 100%|██████████| 340/340 [02:08<00:00,  2.64it/s]\n",
            "Epoch 8 Iter 340: loss 2.508. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]\n",
            "Epoch 9 Iter 340: loss 0.032. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 10 Iter 340: loss 2.534. : 100%|██████████| 340/340 [02:08<00:00,  2.66it/s]\n",
            "Epoch 11 Iter 340: loss 0.030. : 100%|██████████| 340/340 [02:08<00:00,  2.64it/s]\n",
            "Epoch 12 Iter 340: loss 0.043. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 13 Iter 340: loss 0.034. : 100%|██████████| 340/340 [02:08<00:00,  2.64it/s]\n",
            "Epoch 14 Iter 340: loss 0.024. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]\n",
            "Epoch 15 Iter 340: loss 0.073. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]\n",
            "Epoch 16 Iter 340: loss 0.035. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]\n",
            "Epoch 17 Iter 340: loss 2.852. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 18 Iter 340: loss 1.824. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 19 Iter 340: loss 2.271. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]\n",
            "Epoch 20 Iter 340: loss 1.783. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]\n",
            "Epoch 21 Iter 340: loss 0.007. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 22 Iter 340: loss 0.023. : 100%|██████████| 340/340 [02:08<00:00,  2.64it/s]\n",
            "Epoch 23 Iter 340: loss 0.014. : 100%|██████████| 340/340 [02:07<00:00,  2.66it/s]\n",
            "Epoch 24 Iter 340: loss 0.015. : 100%|██████████| 340/340 [02:08<00:00,  2.64it/s]\n",
            "Epoch 25 Iter 340: loss 0.028. : 100%|██████████| 340/340 [02:08<00:00,  2.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_maxvit = torch.optim.Adam(maxvit_model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 5 epochs\n",
        "scheduler_maxvit = torch.optim.lr_scheduler.StepLR(optimizer_maxvit, step_size=7, gamma=0.1)\n",
        "\n",
        "writer_maxvit = utils.make_tboard_logs(\"maxvit_tiny_finetuned\")\n",
        "maxvit_train_loss, maxvit_val_loss, maxvit_loss_iters, maxvit_valid_acc = train_model(\n",
        "        model=maxvit_model, optimizer=optimizer_maxvit, scheduler=scheduler_maxvit, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_maxvit,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CrruADSUsYOM"
      },
      "source": [
        "With a final accuracy of 91.26% the MaxVit Transformer based model (default size) still loses to the ConvNeXt (tiny) with 92.43% accuray. We went with MaxVit Transformer model because it had one of the highest performances for transformer based models on benchmarking sites. We thought the transformer could have been a better performer but we were wrong and ConvNeXt still beat the transformer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq3a8Y9w9Afj"
      },
      "source": [
        "#### **Experiment 2:**\n",
        "Try to get the best performance possible on this dataset\n",
        " - Fine-tune a pretrained neural network of your choice for classification.\n",
        " - Select a good training recipe: augmentations, optimizer, learning rate scheduling, classifier, loss function, ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgCGTeOo9Afj"
      },
      "source": [
        "Because we saw that ConvNeXt is doing the best we try a bigger ConvNeXT Model and higher learning rate decay because it converged pretty fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-led4rw9Afj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "convnext_model = models.convnext_base(pretrained=True)\n",
        "convnext_model.classifier[2] = nn.Linear(1024, NUM_LABELS)\n",
        "\n",
        "convnext_model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDXti1A69Afl"
      },
      "outputs": [],
      "source": [
        "# classification loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_convnext = torch.optim.Adam(convnext_model.parameters(), lr=3e-4)\n",
        "\n",
        "# Decay LR by a factor of 0.5 every 5 epochs\n",
        "scheduler_convnext = torch.optim.lr_scheduler.StepLR(optimizer_convnext, step_size=5, gamma=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA_H9jiaMDgr",
        "outputId": "9feec5a4-0183-4c6f-f5dc-1777fe409045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 Iter 102: loss 2.597. : 100%|██████████| 102/102 [02:13<00:00,  1.31s/it]\n",
            "Epoch 2 Iter 102: loss 0.741. : 100%|██████████| 102/102 [02:14<00:00,  1.32s/it]\n",
            "Epoch 3 Iter 102: loss 0.440. : 100%|██████████| 102/102 [02:25<00:00,  1.42s/it]\n",
            "Epoch 4 Iter 102: loss 0.223. : 100%|██████████| 102/102 [02:14<00:00,  1.32s/it]\n",
            "Epoch 5 Iter 102: loss 0.251. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 6 Iter 102: loss 0.047. : 100%|██████████| 102/102 [02:14<00:00,  1.32s/it]\n",
            "Epoch 7 Iter 102: loss 1.827. : 100%|██████████| 102/102 [02:15<00:00,  1.32s/it]\n",
            "Epoch 8 Iter 102: loss 0.055. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 9 Iter 102: loss 1.640. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 10 Iter 102: loss 0.036. : 100%|██████████| 102/102 [02:14<00:00,  1.32s/it]\n",
            "Epoch 11 Iter 102: loss 0.076. : 100%|██████████| 102/102 [02:15<00:00,  1.32s/it]\n",
            "Epoch 12 Iter 102: loss 2.137. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 13 Iter 102: loss 0.015. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 14 Iter 102: loss 0.012. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 15 Iter 102: loss 0.035. : 100%|██████████| 102/102 [02:15<00:00,  1.32s/it]\n",
            "Epoch 16 Iter 102: loss 1.457. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 17 Iter 102: loss 0.017. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 18 Iter 102: loss 0.011. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 19 Iter 102: loss 0.017. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 20 Iter 102: loss 0.021. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 21 Iter 102: loss 0.009. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 22 Iter 102: loss 0.015. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 23 Iter 102: loss 0.015. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 24 Iter 102: loss 0.030. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]\n",
            "Epoch 25 Iter 102: loss 2.245. : 100%|██████████| 102/102 [02:15<00:00,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "writer_convnext = utils.make_tboard_logs(\"convnext_base_finetuned\")\n",
        "convnext_train_loss, convnext_val_loss, convnext_loss_iters, convnext_valid_acc = train_model(\n",
        "        model=convnext_model, optimizer=optimizer_convnext, scheduler=scheduler_convnext, criterion=criterion,\n",
        "        train_loader=train_loader, valid_loader=test_loader, num_epochs=NUM_EPOCHS, tboard=writer_convnext,\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tiny ConvNeXt params: 28589128\n",
            "Base ConvNeXt params: 88591464\n",
            "Ratio base to tiny: 3.098781606770238\n"
          ]
        }
      ],
      "source": [
        "convnext_tiny = models.convnext_tiny()\n",
        "tiny_params = utils.count_parameters(convnext_tiny)\n",
        "\n",
        "convnext_base = models.convnext_base()\n",
        "base_params = utils.count_parameters(convnext_base)\n",
        "\n",
        "print(\"Tiny ConvNeXt params:\", tiny_params)\n",
        "print(\"Base ConvNeXt params:\",base_params)\n",
        "print(\"Ratio base to tiny:\", base_params/tiny_params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the Tensorboard we can see that the base model of ConvNeXt is 0.83% better. With regard to the parameter amount this is the trade off for a little bit more accuracy while having a model that is 3 times the size. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a3c029b62e9af0c14c8d20940fd275a7433002acd075126a9d5c47011a54832f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
