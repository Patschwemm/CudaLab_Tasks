{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Vision Sytems: Session 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today:\n",
    "\n",
    "### 1: Solution Assignment 4\n",
    "### 2: Generative Adversarial Networks\n",
    "### 3: Generating New Clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Announcements and Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading External Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Existing Code\n",
    "\n",
    "If you use code snippets from GitHub or StackOverflow, **cite the original source in a comment or Docstring!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/reference.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Project\n",
    "\n",
    " - Introduced during the final lab session (03.02.2023)\n",
    " - Deadline: March 31st\n",
    " - Submission:\n",
    "   - Codebase\n",
    "   - Illustrative Notebook\n",
    "   - Report (around 8 pages)\n",
    " - Review session:\n",
    "   - Individual group meetings where you explain your progress\n",
    "   - Beggining of March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seminar Vision Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/seminar.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Solution Assignment 4\n",
    "\n",
    "By Gjergj and Jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Generative Adversarial Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Generating New Clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", \"gan\")\n",
    "if not os.path.exists(GAN_LOGS):\n",
    "    os.makedirs(GAN_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(GAN_LOGS)\n",
    "writer = SummaryWriter(GAN_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"imgs\"):\n",
    "    os.makedirs(\"imgs\")\n",
    "shutil.rmtree(\"imgs/training\")\n",
    "if not os.path.exists(\"imgs/training\"):\n",
    "    os.makedirs(\"imgs/training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and Loading Dataset\n",
    "mnist_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Pad(2),\n",
    "        transforms.Normalize(mean=(0.5, ), std=(0.5, ))\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=mnist_tf, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=mnist_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional GAN\n",
    "\n",
    " - Following the DC-GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional block: Conv + Norm + Act + Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, add_norm=True, activation=\"ReLU\", dropout=None):\n",
    "        \"\"\" Module Initializer \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [\"ReLU\", \"LeakyReLU\", \"Sigmoid\", \"Tanh\", None]\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        block = []\n",
    "        block.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride))\n",
    "        if add_norm:\n",
    "            block.append(nn.BatchNorm2d(out_channels))\n",
    "        if activation is not None:\n",
    "            nonlinearity = getattr(nn, activation, nn.ReLU)()\n",
    "            if isinstance(nonlinearity, nn.LeakyReLU):\n",
    "                nonlinearity.negative_slope = 0.2\n",
    "            block.append(nonlinearity)\n",
    "            \n",
    "        if dropout is not None:\n",
    "            block.append(nn.Dropout(dropout))\n",
    "            \n",
    "        self.block =  nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.block(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvTransposeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional block: ConvTranspose + Norm + Act + Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, add_norm=True, activation=\"ReLU\", dropout=None):\n",
    "        \"\"\" Module Initializer \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [\"ReLU\", \"LeakyReLU\", \"Tanh\", None]\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        block = []\n",
    "        block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, stride=stride))\n",
    "        if add_norm:\n",
    "            block.append(nn.BatchNorm2d(out_channels))\n",
    "        if activation is not None:\n",
    "            nonlinearity = getattr(nn, activation, nn.ReLU)()\n",
    "            if isinstance(nonlinearity, nn.LeakyReLU):\n",
    "                nonlinearity.negative_slope = 0.2\n",
    "            block.append(nonlinearity)\n",
    "        if dropout is not None:\n",
    "            block.append(nn.Dropout(dropout))\n",
    "            \n",
    "        self.block =  nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.block(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    \"\"\" Reshaping a vector in a given shape \"\"\"\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" \"\"\"\n",
    "        B, N = x.shape\n",
    "        x = x.view(B, N, 1, 1)\n",
    "        y = x.repeat(1, 1, *self.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully convolutional generator using ReLU activations. \n",
    "    Takes as input a latent vector and outputs a fake sample.\n",
    "       (B, latent_dim, 1, 1)  --> (B, num_channels, 32, 32)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, num_channels=1, base_channels=32):\n",
    "        \"\"\" Model initializer \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(\n",
    "                ConvTransposeBlock(\n",
    "                        in_channels=latent_dim if i == 0 else base_channels * 2 ** (3-i+1),\n",
    "                        out_channels=base_channels * 2 ** (3-i),\n",
    "                        kernel_size=4,\n",
    "                        stride=1 if i == 0 else 2,\n",
    "                        add_norm=True,\n",
    "                        activation=\"ReLU\"\n",
    "                    )\n",
    "                )\n",
    "        layers.append(\n",
    "            ConvTransposeBlock(\n",
    "                    in_channels=base_channels,\n",
    "                    out_channels=num_channels,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    add_norm=False,\n",
    "                    activation=\"Tanh\"\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through generator \"\"\"\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): ConvTransposeBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvTranspose2d(128, 256, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): ConvTransposeBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (2): ConvTransposeBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (3): ConvTransposeBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvTransposeBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(latent_dim=128, num_channels=1, base_channels=32)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" A fully convolutional discriminator using LeakyReLU activations. \n",
    "    Takes as input either a real or fake sample and predicts its autenticity.\n",
    "       (B, num_channels, 32, 32)  -->  (B, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_dim=1, base_channels=32, dropout=0.3):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        super().__init__()  \n",
    "        \n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(\n",
    "                ConvBlock(\n",
    "                        in_channels=in_channels if i == 0 else base_channels * 2 ** i,\n",
    "                        out_channels=base_channels * 2 ** (i + 1),\n",
    "                        kernel_size=4,\n",
    "                        add_norm=True,\n",
    "                        activation=\"LeakyReLU\",\n",
    "                        dropout=dropout,\n",
    "                        stride=2\n",
    "                    )\n",
    "                )\n",
    "        layers.append(\n",
    "                ConvBlock(\n",
    "                        in_channels=base_channels * 16,\n",
    "                        out_channels=out_dim,\n",
    "                        kernel_size=4,\n",
    "                        stride=4,\n",
    "                        add_norm=False,\n",
    "                        activation=\"Sigmoid\"\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        return\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(4, 4), padding=(2, 2))\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(in_channels=1, out_dim=1, base_channels=32)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=128, num_channels=1, base_channels=32)\n",
    "discriminator = Discriminator(in_channels=1, out_dim=1, base_channels=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_img.shape = torch.Size([16, 1, 32, 32])\n",
      "score.shape = torch.Size([16, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "latent = torch.randn(16, 128, 1, 1)\n",
    "fake_img = generator(latent)\n",
    "score = discriminator(fake_img)\n",
    "print(f\"{fake_img.shape = }\")\n",
    "print(f\"{score.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def smooth(f, K=5):\n",
    "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
    "    kernel = np.ones(K) / K\n",
    "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
    "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
    "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
    "    return smooth_f\n",
    "\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params\n",
    "\n",
    "def visualize_progress(loss_iters, train_loss, val_loss):\n",
    "    \"\"\" Visualizing loss and accuracy \"\"\"\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_size_inches(24,5)\n",
    "\n",
    "    smooth_loss = smooth(loss_iters, 31)\n",
    "    ax[0].plot(loss_iters, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[0].plot(smooth_loss, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].set_ylabel(\"CE Loss\")\n",
    "    ax[0].set_yscale(\"log\")\n",
    "    ax[0].set_title(\"Training Progress\")\n",
    "\n",
    "    smooth_loss = smooth(loss_iters, 31)\n",
    "    START = 500\n",
    "    N_ITERS = len(loss_iters)\n",
    "    ax[1].plot(np.arange(START, N_ITERS), loss_iters[START:], c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[1].plot(np.arange(START, N_ITERS), smooth_loss[START:], c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].set_ylabel(\"CE Loss\")\n",
    "    ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_title(f\"Training Progress from Iter {START}\")\n",
    "\n",
    "    epochs = np.arange(len(train_loss)) + 1\n",
    "    ax[2].plot(epochs[1:], train_loss[1:], c=\"red\", label=\"Train Loss\", linewidth=3)\n",
    "    ax[2].plot(epochs[1:], val_loss[1:], c=\"blue\", label=\"Valid Loss\", linewidth=3)\n",
    "    ax[2].legend(loc=\"best\")\n",
    "    ax[2].set_xlabel(\"Epochs\")\n",
    "    ax[2].set_ylabel(\"CE Loss\")\n",
    "    ax[2].set_title(\"Loss Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Class for initializing GAN and training it\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, discriminator, latent_dim=128, writer=None):\n",
    "        \"\"\" Initialzer \"\"\"\n",
    "        assert writer is not None, f\"Tensorboard writer not set...\"\n",
    "    \n",
    "        self.latent_dim = latent_dim\n",
    "        self.writer = writer \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator = generator.to(self.device)\n",
    "        self.discriminator = discriminator.to(self.device)\n",
    "        \n",
    "        self.optim_discriminator = torch.optim.Adam(self.discriminator.parameters(), lr=3e-4, betas=(0.5, 0.9))\n",
    "        self.optim_generator = torch.optim.Adam(self.generator.parameters(), lr=3e-4, betas=(0.5, 0.9))\n",
    "        \n",
    "        # REAL LABEL = 1\n",
    "        # FAKE LABEL = 0\n",
    "        # eps = 1e-10\n",
    "        # self.criterion_d_real = lambda pred: torch.clip(-torch.log(1 - pred + eps), min=-10).mean()\n",
    "        # self.criterion_d_fake = lambda pred: torch.clip(-torch.log(pred + eps), min=-10).mean()\n",
    "        # self.criterion_g = lambda pred: torch.clip(-torch.log(1 - pred + eps), min=-10).mean()\n",
    "        \n",
    "        self.criterion_d_real = lambda pred: F.binary_cross_entropy(pred, torch.ones(pred.shape[0], device=pred.device))\n",
    "        self.criterion_d_fake = lambda pred: F.binary_cross_entropy(pred, torch.zeros(pred.shape[0], device=pred.device))\n",
    "        self.criterion_g = lambda pred: F.binary_cross_entropy(pred, torch.ones(pred.shape[0], device=pred.device))\n",
    "        \n",
    "        self.hist = {\n",
    "            \"d_real\": [],\n",
    "            \"d_fake\": [],\n",
    "            \"g\": []\n",
    "        }\n",
    "        return\n",
    "        \n",
    "    def train_one_step(self, imgs):\n",
    "        \"\"\" \n",
    "        raining both models for one optimization step\n",
    "        \"\"\"\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        # Sample from the latent distribution\n",
    "        B = imgs.shape[0]\n",
    "        latent = torch.randn(B, self.latent_dim, 1, 1).to(self.device)\n",
    "        \n",
    "        # ==== Training Discriminator ====\n",
    "        self.optim_discriminator.zero_grad()\n",
    "        # Get discriminator outputs for the real samples\n",
    "        prediction_real = self.discriminator(imgs)\n",
    "        # Compute the loss function\n",
    "        d_loss_real = self.criterion_d_real(prediction_real.view(B))\n",
    "\n",
    "        # Generating fake samples with the generator\n",
    "        fake_samples = self.generator(latent)\n",
    "        # Get discriminator outputs for the fake samples\n",
    "        prediction_fake_d = self.discriminator(fake_samples.detach())  # why detach?\n",
    "        # Compute the loss function\n",
    "        d_loss_fake = self.criterion_d_fake(prediction_fake_d.view(B))\n",
    "        (d_loss_real + d_loss_fake).backward()\n",
    "        assert fake_samples.shape == imgs.shape\n",
    "        \n",
    "        # optimization step\n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 3.0)\n",
    "        self.optim_discriminator.step()\n",
    "        \n",
    "        # === Train the generator ===\n",
    "        self.optim_generator.zero_grad()\n",
    "        # Get discriminator outputs for the fake samples\n",
    "        prediction_fake_g = self.discriminator(fake_samples)\n",
    "        # Compute the loss function\n",
    "        g_loss = self.criterion_g(prediction_fake_g.view(B))\n",
    "        g_loss.backward()\n",
    "        # optimization step\n",
    "        self.optim_generator.step()\n",
    "        \n",
    "        return d_loss_real, d_loss_fake, g_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, N=64):\n",
    "        \"\"\" Generating a bunch of images using current state of generator \"\"\"\n",
    "        self.generator.eval()\n",
    "        latent = torch.randn(N, self.latent_dim, 1, 1).to(self.device)\n",
    "        imgs = self.generator(latent)\n",
    "        imgs = imgs * 0.5 + 0.5\n",
    "        return imgs\n",
    "        \n",
    "    def train(self, data_loader, N_iters=10000, init_step=0):\n",
    "        \"\"\" Training the models for several iterations \"\"\"\n",
    "        \n",
    "        progress_bar = tqdm(total=N_iters, initial=init_step)\n",
    "        running_d_loss = 0\n",
    "        running_g_loss = 0\n",
    "        \n",
    "        iter_ = 0\n",
    "        for i in range(N_iters):\n",
    "            for real_batch, _ in data_loader:           \n",
    "                real_batch = real_batch.to(self.device)\n",
    "                d_loss_real, d_loss_fake, g_loss = self.train_one_step(imgs=real_batch)\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "                # updating progress bar\n",
    "                progress_bar.set_description(f\"Ep {i+1} Iter {iter_}: D_Loss={round(d_loss.item(),5)}, G_Loss={round(g_loss.item(),5)})\")\n",
    "                \n",
    "                # adding stuff to tensorboard\n",
    "                self.writer.add_scalar(f'Loss/Generator Loss', g_loss.item(), global_step=iter_)\n",
    "                self.writer.add_scalar(f'Loss/Discriminator Loss', d_loss.item(), global_step=iter_)\n",
    "                self.writer.add_scalars(f'Loss/Discriminator Losses', {\n",
    "                        \"Real Images Loss\": d_loss_real.item(),\n",
    "                        \"Fake Images Loss\": d_loss_fake.item(),\n",
    "                    }, global_step=iter_)\n",
    "                self.writer.add_scalars(f'Comb_Loss/Losses', {\n",
    "                            'Discriminator': d_loss.item(),\n",
    "                            'Generator':  g_loss.item()\n",
    "                        }, iter_)    \n",
    "                if(iter_ % 200 == 0):\n",
    "                    imgs = self.generate()\n",
    "                    grid = torchvision.utils.make_grid(imgs, nrow=8)\n",
    "                    self.writer.add_image('images', grid, global_step=iter_)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(os.getcwd(), \"imgs\", \"training\", f\"imgs_{iter_}.png\"))\n",
    "\n",
    "                iter_ = iter_ + 1 \n",
    "                \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(GAN_LOGS)\n",
    "writer = SummaryWriter(GAN_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim=128, num_channels=1, base_channels=32)\n",
    "discriminator = Discriminator(in_channels=1, out_dim=1, base_channels=32)\n",
    "\n",
    "trainer = Trainer(generator=generator, discriminator=discriminator, latent_dim=128, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 Iter 1582: D_Loss=0.50138, G_Loss=4.80441):   0%|          | 0/10000 [02:57<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9c61f0b8b053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-a130eb465115>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, N_iters, init_step)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0miter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mreal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mreal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m     \"\"\"\n\u001b[0;32m-> 2803\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2804\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(data_loader=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "images = []\n",
    "img_path = os.path.join(os.getcwd(), \"imgs\", \"training\")\n",
    "\n",
    "# making list with images and orting by iteration\n",
    "img_list = [img for img in os.listdir(img_path) if \"imgs_\" in img]\n",
    "sorted_imgs = sorted(img_list, key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in sorted_imgs:\n",
    "    images.append(imageio.imread(os.path.join(img_path, img)))\n",
    "imageio.mimsave(os.path.join(img_path, \"progress.gif\"), images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = trainer.generate()\n",
    "grid = torchvision.utils.make_grid(imgs, nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(grid[0].cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    " - Experiment 1:\n",
    "   - Implement a conditional DCGAN model (https://arxiv.org/abs/1411.1784)\n",
    "   - Train the model for conditional generation on the Fashion MNIST dataset\n",
    "   - Requirements:\n",
    "     - Use Tensorboard, WandDB or some other experiment tracker\n",
    "     - Show the capabilities of the model to generate data based on given label\n",
    "     \n",
    " - Experiment 2:\n",
    "    - Implement a fully convolutional DCGAN-like model (https://arxiv.org/abs/1511.06434)\n",
    "    - Train the model on the CelebA dataset to generate new faces\n",
    "    - Requirements:\n",
    "      - Use Tensorboard, WandDB or some other experiment tracker\n",
    "      - Show the capabilities of your model to generate images\n",
    "      - Evaluate and track during training using one quantitative metric (e.g. FID)\n",
    "      \n",
    " - Extra point:\n",
    "    - Extend your VAE from Assignment 5 with Adversarial supervision (Adversarial Autoencoder https://arxiv.org/abs/1511.05644)\n",
    "    - Real images, as well as samples generated by the VAE, are fed to a discriminator in a GAN-like manner\n",
    "    - Use if to generate more realistic CelebA-like images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Due Date**: Wednesday 25th January at 23:59\n",
    "#### Submit it by mail using the subject: **CudaLab: Assignment6 + Group Number**\n",
    "####  Send me the following: Jupyter Notebook after running, Jupyter export as **html**, **Tensorboard Logs**, any other .py files or images used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Next Sessions:**\n",
    " \n",
    "####  **January  20th**: Deep Metric Learning\n",
    "####  **February 3rd**: Semantic Segmentation and Class Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    " - https://www.deeplearningbook.org/\n",
    " - https://towardsdatascience.com/all-you-want-to-know-about-deep-learning-8d68dcffc258\n",
    " - https://github.com/soumith/ganhacks\n",
    " - https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=alert style=\"background-color:#F5F5F5; border-color:#C8C8C8\">\n",
    "    <b>Angel Villar-Corrales</b><br>\n",
    "    <ul>\n",
    "       <li> <b>Email</b>: villar@ais.uni-bonn.de\n",
    "       <li> <b>Website</b>: angelvillarcorrales.com\n",
    "    </ul>\n",
    "</div>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
